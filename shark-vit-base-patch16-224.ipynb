{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBn6VzY6xrkxP7u1ljSsQI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/Shark-identification-1/blob/main/shark-vit-base-patch16-224.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvoQBtPU5s8m"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q transformers datasets torchvision evaluate\n",
        "!pip install wandb\n",
        "\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "# المسار إلى البيانات\n",
        "data_dir = \"/content/Shark_project_split\"\n",
        "\n",
        "# التحويلات المبدئية (تصغير الصور وتحويلها إلى Tensor)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# تحميل البيانات\n",
        "dataset = ImageFolder(data_dir, transform=transform)\n",
        "\n",
        "# تقسيم إلى تدريب واختبار\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\n",
        "#---\n",
        "\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, TrainingArguments, Trainer, ViTImageProcessor\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset as HFDataset\n",
        "\n",
        "# استخلاص الخصائص\n",
        "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\") # Deprecated\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "\n",
        "# تعريف نموذج التصنيف\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224\",\n",
        "    num_labels=len(dataset.classes),\n",
        "    id2label={str(i): c for i, c in enumerate(dataset.classes)},\n",
        "    label2id={c: str(i) for i, c in enumerate(dataset.classes)},\n",
        "    ignore_mismatched_sizes=True # Add this argument to ignore the size mismatch in the classifier layer\n",
        ")\n",
        "\n",
        "#----\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def transform_example(example):\n",
        "    image = example['image']\n",
        "    encoding = feature_extractor(images=image, return_tensors=\"pt\")\n",
        "    encoding['label'] = example['label']\n",
        "    return encoding\n",
        "\n",
        "# تحويل بيانات PyTorch إلى Dataset من نوع Hugging Face\n",
        "def convert_to_hf_dataset(torch_dataset):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for img, label in torch_dataset:\n",
        "        images.append(img.permute(1, 2, 0).numpy())  # Convert to HWC\n",
        "        labels.append(label)\n",
        "    return HFDataset.from_dict({\"image\": images, \"label\": labels})\n",
        "\n",
        "hf_train = convert_to_hf_dataset(train_dataset).with_transform(transform_example)\n",
        "hf_val = convert_to_hf_dataset(val_dataset).with_transform(transform_example)\n",
        "\n",
        "\n",
        "#----\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit-shark-classifier2\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    eval_strategy=\"epoch\", # Changed from evaluation_strategy\n",
        "    save_strategy=\"epoch\",\n",
        "    num_train_epochs=5,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "from evaluate import load\n",
        "accuracy = load(\"accuracy\")\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = np.argmax(p.predictions, axis=1)\n",
        "    return accuracy.compute(predictions=preds, references=p.label_ids)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_train,\n",
        "    eval_dataset=hf_val,\n",
        "    image_processor=feature_extractor,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 165LwqivtdzeXwMaj2VeGzgspqdnOiyrq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqubQO7u6q0X",
        "outputId": "f11a7da4-bef2-4c83-92a1-dd6148167a57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=165LwqivtdzeXwMaj2VeGzgspqdnOiyrq\n",
            "From (redirected): https://drive.google.com/uc?id=165LwqivtdzeXwMaj2VeGzgspqdnOiyrq&confirm=t&uuid=30d20897-6cef-4d8a-a625-a84d6a25e28f\n",
            "To: /content/Shark_project_split.zip\n",
            "100% 139M/139M [00:03<00:00, 40.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/Shark_project_split.zip\" -d \"/content/Shark_project_split\""
      ],
      "metadata": {
        "id": "KcT08f-V6zQL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}