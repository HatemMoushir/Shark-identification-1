{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND/JEFcrvnYdI5zkMsEZ+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/Shark-identification-1/blob/main/Shark%20identification_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… ØªØ«Ø¨ÙŠØª icrawler\n",
        "!pip install -q icrawler\n",
        "\n",
        "# âœ… Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª\n",
        "import os\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# âœ… Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±\n",
        "save_dir = \"shark_fins\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# âœ… Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ (Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠ + Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ)\n",
        "shark_species = [\n",
        "    (\"Scalloped hammerhead\", \"Sphyrna lewini\"),\n",
        "    (\"Great hammerhead\", \"Sphyrna mokarran\"),\n",
        "    (\"Oceanic whitetip shark\", \"Carcharhinus longimanus\"),\n",
        "    (\"Tiger shark\", \"Galeocerdo cuvier\"),\n",
        "    (\"Silky shark\", \"Carcharhinus falciformis\"),\n",
        "    (\"Blacktip shark\", \"Carcharhinus limbatus\"),\n",
        "    (\"Dusky shark\", \"Carcharhinus obscurus\"),\n",
        "    (\"Grey reef shark\", \"Carcharhinus amblyrhynchos\"),\n",
        "    (\"Thresher shark\", \"Alopias vulpinus\"),\n",
        "    (\"Shortfin mako\", \"Isurus oxyrinchus\")\n",
        "]\n",
        "\n",
        "# âœ… Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚Ø©\n",
        "trusted_sources = [\n",
        "    \"site:wikipedia.org\",\n",
        "    \"site:inaturalist.org\",\n",
        "    \"site:marinespecies.org\",\n",
        "    \"site:noaa.gov\",\n",
        "    \"site:arkive.org\",\n",
        "    \"site:shark-references.com\"\n",
        "]\n",
        "\n",
        "# âœ… Ø¯Ø§Ù„Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù…Ù† Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… icrawler\n",
        "def download_images(query, species_folder, limit=500):\n",
        "    os.makedirs(species_folder, exist_ok=True)\n",
        "    try:\n",
        "        google_crawler = GoogleImageCrawler(storage={'root_dir': species_folder})\n",
        "        google_crawler.crawl(keyword=query, max_num=limit, file_idx_offset='auto')\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ ØµÙˆØ± Ù„Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…: {query}\\nØ§Ù„Ø®Ø·Ø£: {e}\")\n",
        "\n",
        "# âœ… ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù…ÙŠÙ„ Ù„ÙƒÙ„ Ù†ÙˆØ¹ Ù‚Ø±Ø´\n",
        "for common, scientific in shark_species:\n",
        "    folder_name = os.path.join(save_dir, common.replace(\" \", \"_\"))\n",
        "    print(f\"â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ù„Ù€: {common} ({scientific})...\")\n",
        "    for site in trusted_sources:\n",
        "        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø´Ø§Ø¦Ø¹\n",
        "        query_common = f\"{common} dorsal fin {site}\"\n",
        "        download_images(query_common, folder_name)\n",
        "        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø¨Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ø¹Ù„Ù…ÙŠ\n",
        "        query_scientific = f\"{scientific} dorsal fin {site}\"\n",
        "        download_images(query_scientific, folder_name)\n",
        "\n",
        "print(\"âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±.\")"
      ],
      "metadata": {
        "id": "0THWWZqbyQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "source_dir = \"shark_fins\"\n",
        "target_dir = \"shark_dataset\"\n",
        "split_ratio = 0.8  # 80% ØªØ¯Ø±ÙŠØ¨ØŒ 20% ØªØ­Ù‚Ù‚\n",
        "\n",
        "for species in os.listdir(source_dir):\n",
        "    src_path = os.path.join(source_dir, species)\n",
        "    imgs = os.listdir(src_path)\n",
        "    random.shuffle(imgs)\n",
        "\n",
        "    split_index = int(len(imgs) * split_ratio)\n",
        "    train_imgs = imgs[:split_index]\n",
        "    val_imgs = imgs[split_index:]\n",
        "\n",
        "    for phase, image_list in [('train', train_imgs), ('val', val_imgs)]:\n",
        "        dest_folder = os.path.join(target_dir, phase, species)\n",
        "        os.makedirs(dest_folder, exist_ok=True)\n",
        "        for img_file in image_list:\n",
        "            shutil.copy(os.path.join(src_path, img_file), os.path.join(dest_folder, img_file))"
      ],
      "metadata": {
        "id": "kYuc5BDy73pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_folder_size(path):\n",
        "    total_size = 0\n",
        "    for dirpath, _, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(fp):\n",
        "                total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 * 1024)  # Ø¨Ø§Ù„Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª\n",
        "\n",
        "train_size = get_folder_size(\"shark_dataset/train\")\n",
        "val_size = get_folder_size(\"shark_dataset/val\")\n",
        "\n",
        "print(f\"ğŸ“¦ Ø­Ø¬Ù… train: {train_size:.2f} MB\")\n",
        "print(f\"ğŸ“¦ Ø­Ø¬Ù… val  : {val_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsyhVKiT7_BM",
        "outputId": "5d1922f7-02fd-474a-9aa7-a930cff961d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Ø­Ø¬Ù… train: 278.73 MB\n",
            "ğŸ“¦ Ø­Ø¬Ù… val  : 81.55 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_folders(base_path):\n",
        "    print(f\"\\nğŸ“‚ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯: {base_path}\\n\" + \"-\"*40)\n",
        "    total_images = 0\n",
        "    for species in sorted(os.listdir(base_path)):\n",
        "        folder_path = os.path.join(base_path, species)\n",
        "        if os.path.isdir(folder_path):\n",
        "            num_images = len([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            total_images += num_images\n",
        "            print(f\"{species:<30} : {num_images} ØµÙˆØ±Ø©\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"ğŸ“Š Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_images} ØµÙˆØ±Ø©\\n\")\n",
        "\n",
        "count_images_in_folders(\"shark_dataset/train\")\n",
        "count_images_in_folders(\"shark_dataset/val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5of7HdY18NzI",
        "outputId": "1fbb0e1e-093c-4152-dfb9-79f7e688a626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“‚ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯: shark_dataset/train\n",
            "----------------------------------------\n",
            "Blacktip_shark                 : 0 ØµÙˆØ±Ø©\n",
            "Dusky_shark                    : 0 ØµÙˆØ±Ø©\n",
            "Great_hammerhead               : 251 ØµÙˆØ±Ø©\n",
            "Grey_reef_shark                : 0 ØµÙˆØ±Ø©\n",
            "Oceanic_whitetip_shark         : 245 ØµÙˆØ±Ø©\n",
            "Scalloped_hammerhead           : 254 ØµÙˆØ±Ø©\n",
            "Shortfin_mako                  : 0 ØµÙˆØ±Ø©\n",
            "Silky_shark                    : 0 ØµÙˆØ±Ø©\n",
            "Thresher_shark                 : 0 ØµÙˆØ±Ø©\n",
            "Tiger_shark                    : 210 ØµÙˆØ±Ø©\n",
            "----------------------------------------\n",
            "ğŸ“Š Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: 960 ØµÙˆØ±Ø©\n",
            "\n",
            "\n",
            "ğŸ“‚ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯: shark_dataset/val\n",
            "----------------------------------------\n",
            "Blacktip_shark                 : 0 ØµÙˆØ±Ø©\n",
            "Dusky_shark                    : 0 ØµÙˆØ±Ø©\n",
            "Great_hammerhead               : 63 ØµÙˆØ±Ø©\n",
            "Grey_reef_shark                : 0 ØµÙˆØ±Ø©\n",
            "Oceanic_whitetip_shark         : 62 ØµÙˆØ±Ø©\n",
            "Scalloped_hammerhead           : 64 ØµÙˆØ±Ø©\n",
            "Shortfin_mako                  : 0 ØµÙˆØ±Ø©\n",
            "Silky_shark                    : 0 ØµÙˆØ±Ø©\n",
            "Thresher_shark                 : 0 ØµÙˆØ±Ø©\n",
            "Tiger_shark                    : 53 ØµÙˆØ±Ø©\n",
            "----------------------------------------\n",
            "ğŸ“Š Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: 242 ØµÙˆØ±Ø©\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def remove_empty_folders(base_path):\n",
        "    for species in os.listdir(base_path):\n",
        "        folder_path = os.path.join(base_path, species)\n",
        "        if os.path.isdir(folder_path) and len(os.listdir(folder_path)) == 0:\n",
        "            shutil.rmtree(folder_path)\n",
        "            print(f\"ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: {folder_path}\")\n",
        "\n",
        "remove_empty_folders(\"shark_dataset/train\")\n",
        "remove_empty_folders(\"shark_dataset/val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz15scj98xtx",
        "outputId": "afeed2e2-383b-4022-cb54-3497599a84a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/train/Silky_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/train/Shortfin_mako\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/train/Blacktip_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/train/Grey_reef_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/train/Thresher_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/train/Dusky_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/val/Silky_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/val/Shortfin_mako\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/val/Blacktip_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/val/Grey_reef_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/val/Thresher_shark\n",
            "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„ÙØ§Ø±Øº: shark_dataset/val/Dusky_shark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# âœ… Ø§Ù„ØªØ­ÙˆÙŠÙ„Ø§Øª\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# âœ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n",
        "train_dir = \"shark_dataset/train\"\n",
        "val_dir = \"shark_dataset/val\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# âœ… ØªØ­Ù…ÙŠÙ„ ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# âœ… ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø·Ø¨Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©\n",
        "num_classes = len(train_dataset.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# âœ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# âœ… Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n",
        "for epoch in range(5):  # Ø¹Ø¯Ø¯ Ù‚Ù„ÙŠÙ„ Ù„Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ø£ÙˆÙ„ÙŠØ©\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"âœ… Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NibtzYqi9LyL",
        "outputId": "669d7c09-bdaa-48e5-9081-9d3eccc67f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:01<00:00, 44.4MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 1, Loss: 42.9185\n",
            "âœ… Epoch 2, Loss: 40.3105\n",
            "âœ… Epoch 3, Loss: 39.2322\n",
            "âœ… Epoch 4, Loss: 37.9232\n",
            "âœ… Epoch 5, Loss: 37.1190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Ø§Ù„ØªÙ‚ÙŠÙŠÙ…\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"ğŸ¯ Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ­Ù‚Ù‚: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAhW8e8R_92u",
        "outputId": "33c8dd3b-410a-46b2-9567-0b08523dc74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¯ Ø¯Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„ØªØ­Ù‚Ù‚: 33.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"shark_fins_raw\", 'zip', \"shark_fins\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C5TJs3u04N1S",
        "outputId": "4987aecf-b248-4dad-e02e-8a82c2a82b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/shark_fins_raw.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}