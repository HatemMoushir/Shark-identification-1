{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyND/JEFcrvnYdI5zkMsEZ+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/Shark-identification-1/blob/main/Shark%20identification_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ تثبيت icrawler\n",
        "!pip install -q icrawler\n",
        "\n",
        "# ✅ استيراد المكتبات\n",
        "import os\n",
        "from icrawler.builtin import GoogleImageCrawler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ✅ إنشاء مجلد لحفظ الصور\n",
        "save_dir = \"shark_fins\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# ✅ قائمة الأنواع (الاسم الإنجليزي + الاسم العلمي)\n",
        "shark_species = [\n",
        "    (\"Scalloped hammerhead\", \"Sphyrna lewini\"),\n",
        "    (\"Great hammerhead\", \"Sphyrna mokarran\"),\n",
        "    (\"Oceanic whitetip shark\", \"Carcharhinus longimanus\"),\n",
        "    (\"Tiger shark\", \"Galeocerdo cuvier\"),\n",
        "    (\"Silky shark\", \"Carcharhinus falciformis\"),\n",
        "    (\"Blacktip shark\", \"Carcharhinus limbatus\"),\n",
        "    (\"Dusky shark\", \"Carcharhinus obscurus\"),\n",
        "    (\"Grey reef shark\", \"Carcharhinus amblyrhynchos\"),\n",
        "    (\"Thresher shark\", \"Alopias vulpinus\"),\n",
        "    (\"Shortfin mako\", \"Isurus oxyrinchus\")\n",
        "]\n",
        "\n",
        "# ✅ المواقع الموثوقة\n",
        "trusted_sources = [\n",
        "    \"site:wikipedia.org\",\n",
        "    \"site:inaturalist.org\",\n",
        "    \"site:marinespecies.org\",\n",
        "    \"site:noaa.gov\",\n",
        "    \"site:arkive.org\",\n",
        "    \"site:shark-references.com\"\n",
        "]\n",
        "\n",
        "# ✅ دالة لتحميل الصور من استعلام معين باستخدام icrawler\n",
        "def download_images(query, species_folder, limit=500):\n",
        "    os.makedirs(species_folder, exist_ok=True)\n",
        "    try:\n",
        "        google_crawler = GoogleImageCrawler(storage={'root_dir': species_folder})\n",
        "        google_crawler.crawl(keyword=query, max_num=limit, file_idx_offset='auto')\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ فشل تحميل صور للاستعلام: {query}\\nالخطأ: {e}\")\n",
        "\n",
        "# ✅ تنفيذ التحميل لكل نوع قرش\n",
        "for common, scientific in shark_species:\n",
        "    folder_name = os.path.join(save_dir, common.replace(\" \", \"_\"))\n",
        "    print(f\"⬇️ تحميل الصور لـ: {common} ({scientific})...\")\n",
        "    for site in trusted_sources:\n",
        "        # تحميل الصور بالاسم الشائع\n",
        "        query_common = f\"{common} dorsal fin {site}\"\n",
        "        download_images(query_common, folder_name)\n",
        "        # تحميل الصور بالاسم العلمي\n",
        "        query_scientific = f\"{scientific} dorsal fin {site}\"\n",
        "        download_images(query_scientific, folder_name)\n",
        "\n",
        "print(\"✅ تم الانتهاء من تحميل الصور.\")"
      ],
      "metadata": {
        "id": "0THWWZqbyQE4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "source_dir = \"shark_fins\"\n",
        "target_dir = \"shark_dataset\"\n",
        "split_ratio = 0.8  # 80% تدريب، 20% تحقق\n",
        "\n",
        "for species in os.listdir(source_dir):\n",
        "    src_path = os.path.join(source_dir, species)\n",
        "    imgs = os.listdir(src_path)\n",
        "    random.shuffle(imgs)\n",
        "\n",
        "    split_index = int(len(imgs) * split_ratio)\n",
        "    train_imgs = imgs[:split_index]\n",
        "    val_imgs = imgs[split_index:]\n",
        "\n",
        "    for phase, image_list in [('train', train_imgs), ('val', val_imgs)]:\n",
        "        dest_folder = os.path.join(target_dir, phase, species)\n",
        "        os.makedirs(dest_folder, exist_ok=True)\n",
        "        for img_file in image_list:\n",
        "            shutil.copy(os.path.join(src_path, img_file), os.path.join(dest_folder, img_file))"
      ],
      "metadata": {
        "id": "kYuc5BDy73pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_folder_size(path):\n",
        "    total_size = 0\n",
        "    for dirpath, _, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            if os.path.isfile(fp):\n",
        "                total_size += os.path.getsize(fp)\n",
        "    return total_size / (1024 * 1024)  # بالميجابايت\n",
        "\n",
        "train_size = get_folder_size(\"shark_dataset/train\")\n",
        "val_size = get_folder_size(\"shark_dataset/val\")\n",
        "\n",
        "print(f\"📦 حجم train: {train_size:.2f} MB\")\n",
        "print(f\"📦 حجم val  : {val_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsyhVKiT7_BM",
        "outputId": "5d1922f7-02fd-474a-9aa7-a930cff961d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📦 حجم train: 278.73 MB\n",
            "📦 حجم val  : 81.55 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_folders(base_path):\n",
        "    print(f\"\\n📂 تحليل المجلد: {base_path}\\n\" + \"-\"*40)\n",
        "    total_images = 0\n",
        "    for species in sorted(os.listdir(base_path)):\n",
        "        folder_path = os.path.join(base_path, species)\n",
        "        if os.path.isdir(folder_path):\n",
        "            num_images = len([f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
        "            total_images += num_images\n",
        "            print(f\"{species:<30} : {num_images} صورة\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"📊 الإجمالي: {total_images} صورة\\n\")\n",
        "\n",
        "count_images_in_folders(\"shark_dataset/train\")\n",
        "count_images_in_folders(\"shark_dataset/val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5of7HdY18NzI",
        "outputId": "1fbb0e1e-093c-4152-dfb9-79f7e688a626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📂 تحليل المجلد: shark_dataset/train\n",
            "----------------------------------------\n",
            "Blacktip_shark                 : 0 صورة\n",
            "Dusky_shark                    : 0 صورة\n",
            "Great_hammerhead               : 251 صورة\n",
            "Grey_reef_shark                : 0 صورة\n",
            "Oceanic_whitetip_shark         : 245 صورة\n",
            "Scalloped_hammerhead           : 254 صورة\n",
            "Shortfin_mako                  : 0 صورة\n",
            "Silky_shark                    : 0 صورة\n",
            "Thresher_shark                 : 0 صورة\n",
            "Tiger_shark                    : 210 صورة\n",
            "----------------------------------------\n",
            "📊 الإجمالي: 960 صورة\n",
            "\n",
            "\n",
            "📂 تحليل المجلد: shark_dataset/val\n",
            "----------------------------------------\n",
            "Blacktip_shark                 : 0 صورة\n",
            "Dusky_shark                    : 0 صورة\n",
            "Great_hammerhead               : 63 صورة\n",
            "Grey_reef_shark                : 0 صورة\n",
            "Oceanic_whitetip_shark         : 62 صورة\n",
            "Scalloped_hammerhead           : 64 صورة\n",
            "Shortfin_mako                  : 0 صورة\n",
            "Silky_shark                    : 0 صورة\n",
            "Thresher_shark                 : 0 صورة\n",
            "Tiger_shark                    : 53 صورة\n",
            "----------------------------------------\n",
            "📊 الإجمالي: 242 صورة\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "def remove_empty_folders(base_path):\n",
        "    for species in os.listdir(base_path):\n",
        "        folder_path = os.path.join(base_path, species)\n",
        "        if os.path.isdir(folder_path) and len(os.listdir(folder_path)) == 0:\n",
        "            shutil.rmtree(folder_path)\n",
        "            print(f\"🗑️ حذف المجلد الفارغ: {folder_path}\")\n",
        "\n",
        "remove_empty_folders(\"shark_dataset/train\")\n",
        "remove_empty_folders(\"shark_dataset/val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz15scj98xtx",
        "outputId": "afeed2e2-383b-4022-cb54-3497599a84a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🗑️ حذف المجلد الفارغ: shark_dataset/train/Silky_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/train/Shortfin_mako\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/train/Blacktip_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/train/Grey_reef_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/train/Thresher_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/train/Dusky_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/val/Silky_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/val/Shortfin_mako\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/val/Blacktip_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/val/Grey_reef_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/val/Thresher_shark\n",
            "🗑️ حذف المجلد الفارغ: shark_dataset/val/Dusky_shark\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ✅ التحويلات\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# ✅ تحميل البيانات\n",
        "train_dir = \"shark_dataset/train\"\n",
        "val_dir = \"shark_dataset/val\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=transform)\n",
        "val_dataset = datasets.ImageFolder(val_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# ✅ تحميل ResNet18\n",
        "model = models.resnet18(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# ✅ تعديل الطبقة النهائية\n",
        "num_classes = len(train_dataset.classes)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "# ✅ التدريب\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# ✅ حلقة التدريب\n",
        "for epoch in range(5):  # عدد قليل للتجربة الأولية\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"✅ Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NibtzYqi9LyL",
        "outputId": "669d7c09-bdaa-48e5-9081-9d3eccc67f88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:01<00:00, 44.4MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Epoch 1, Loss: 42.9185\n",
            "✅ Epoch 2, Loss: 40.3105\n",
            "✅ Epoch 3, Loss: 39.2322\n",
            "✅ Epoch 4, Loss: 37.9232\n",
            "✅ Epoch 5, Loss: 37.1190\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ التقييم\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"🎯 دقة النموذج على مجموعة التحقق: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAhW8e8R_92u",
        "outputId": "33c8dd3b-410a-46b2-9567-0b08523dc74a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 دقة النموذج على مجموعة التحقق: 33.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "shutil.make_archive(\"shark_fins_raw\", 'zip', \"shark_fins\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C5TJs3u04N1S",
        "outputId": "4987aecf-b248-4dad-e02e-8a82c2a82b7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/shark_fins_raw.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}