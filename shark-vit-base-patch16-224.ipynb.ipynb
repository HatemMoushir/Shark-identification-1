{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu0uZyhU+I/YXOmHmuBd7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HatemMoushir/Shark-identification-1/blob/main/shark-vit-base-patch16-224.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 165LwqivtdzeXwMaj2VeGzgspqdnOiyrq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqubQO7u6q0X",
        "outputId": "f11a7da4-bef2-4c83-92a1-dd6148167a57"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=165LwqivtdzeXwMaj2VeGzgspqdnOiyrq\n",
            "From (redirected): https://drive.google.com/uc?id=165LwqivtdzeXwMaj2VeGzgspqdnOiyrq&confirm=t&uuid=30d20897-6cef-4d8a-a625-a84d6a25e28f\n",
            "To: /content/Shark_project_split.zip\n",
            "100% 139M/139M [00:03<00:00, 40.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/Shark_project_split.zip\" -d \"/content/Shark_project_split\""
      ],
      "metadata": {
        "id": "KcT08f-V6zQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#✅ تثبيت المكتبات المطلوبة\n",
        "\n",
        "!pip install -q datasets transformers evaluate torchvision\n",
        "\n",
        "#✅ الاستيراد\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import Dataset, DatasetDict, Image as HFImage # Import HF Image feature\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from PIL import Image as PILImage # Import PIL Image separately\n",
        "\n",
        "#✅ تحميل وتحويل الصور إلى تنسيق DatasetDict\n",
        "\n",
        "def convert_imagefolder_to_datasetdict(data_dir):\n",
        "    dataset_splits = {}\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        path = os.path.join(data_dir, split)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"Warning: Split directory not found: {path}\")\n",
        "            continue\n",
        "\n",
        "        # Use ImageFolder to load images and labels\n",
        "        imagefolder_dataset = ImageFolder(path)\n",
        "\n",
        "        # Extract image paths and labels\n",
        "        image_paths = [img_path for img_path, label in imagefolder_dataset.imgs]\n",
        "        labels = [label for img_path, label in imagefolder_dataset.imgs]\n",
        "        class_names = imagefolder_dataset.classes\n",
        "\n",
        "        # Create Hugging Face Dataset from paths and labels\n",
        "        dataset_splits[split] = Dataset.from_dict({\n",
        "            'image': image_paths,\n",
        "            'label': labels\n",
        "        })\n",
        "\n",
        "        # Cast the 'image' column to the Hugging Face Image feature\n",
        "        # This will load images as PIL objects when accessed\n",
        "        dataset_splits[split] = dataset_splits[split].cast_column(\"image\", HFImage())\n",
        "\n",
        "    return DatasetDict(dataset_splits), class_names\n",
        "\n",
        "# Define the data directory where your split dataset is located\n",
        "data_dir = \"/content/Shark_project_split\"\n",
        "dataset_dict, class_names = convert_imagefolder_to_datasetdict(data_dir)\n",
        "\n",
        "# Print information about the loaded dataset\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(f\"Number of classes: {len(class_names)}\")\n",
        "print(f\"Class names: {class_names}\")\n",
        "print(dataset_dict)\n",
        "\n",
        "#✅ تحميل الـ Image Processor (المعالج)\n",
        "\n",
        "# ViTFeatureExtractor is deprecated; use ViTImageProcessor instead.\n",
        "# When initializing ViTImageProcessor, it's designed to automatically get\n",
        "# the correct mean and std from the pre-trained model's configuration.\n",
        "image_processor = ViTImageProcessor.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k'\n",
        ")\n",
        "\n",
        "#✅ معالجة الصور داخل دالة تحويل (Applied with .map)\n",
        "\n",
        "def transform(examples):\n",
        "    # The 'image' column now contains loaded PIL Images because of cast_column(\"image\", Image())\n",
        "    # We ensure images are converted to RGB as some datasets might contain grayscale or RGBA\n",
        "    # ViT models typically expect 3-channel RGB images.\n",
        "    images = [img.convert(\"RGB\") if img.mode != \"RGB\" else img for img in examples['image']]\n",
        "\n",
        "    # Process a batch of images using the image_processor\n",
        "    # The processor handles resizing, normalization (using its internal mean/std), and tensor conversion.\n",
        "    inputs = image_processor(images, return_tensors='pt')\n",
        "    inputs['labels'] = examples['label'] # Add labels to the processed batch\n",
        "    return inputs\n",
        "\n",
        "# Apply the transformation using .map\n",
        "# This will apply the `transform` function to each batch of examples in the dataset splits.\n",
        "print(\"\\nApplying transformations to the dataset...\")\n",
        "dataset_dict = dataset_dict.map(transform, batched=True)\n",
        "print(\"Transformations applied!\")\n",
        "\n",
        "#✅ حذف العمود الأصلي (اختياري ولكنه ممارسة جيدة لتوفير الذاكرة)\n",
        "\n",
        "dataset_dict = dataset_dict.remove_columns('image')\n",
        "print(\"Original 'image' column removed.\")\n",
        "print(dataset_dict) # Display the dataset structure after transformation\n",
        "\n",
        "#✅ تحميل النموذج\n",
        "\n",
        "# Load the ViT model for image classification\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224-in21k',\n",
        "    num_labels=len(class_names), # Set the number of output labels to match your dataset\n",
        "    id2label={i: label for i, label in enumerate(class_names)}, # Map label IDs to names\n",
        "    label2id={label: i for i, label in enumerate(class_names)}, # Map label names to IDs\n",
        "    ignore_mismatched_sizes=True # Important: Allows loading a pre-trained head with a different number of output neurons\n",
        "                                # and then reinitializing it for your specific num_labels.\n",
        ")\n",
        "print(\"\\nModel loaded successfully!\")\n",
        "\n",
        "#✅ إعدادات التدريب\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./vit-shark\", # Directory to save checkpoints and logs\n",
        "    per_device_train_batch_size=8, # Batch size per GPU/CPU for training\n",
        "    per_device_eval_batch_size=8,  # Batch size per GPU/CPU for evaluation\n",
        "    eval_strategy=\"epoch\",   # Evaluate at the end of each epoch\n",
        "    save_strategy=\"epoch\",         # Save model checkpoint at the end of each epoch\n",
        "    num_train_epochs=5,            # Total number of training epochs\n",
        "    logging_steps=10,              # Log metrics every 10 steps\n",
        "    save_total_limit=2,            # Only keep the last 2 saved checkpoints\n",
        "    load_best_model_at_end=True,   # Load the best model (based on metric_for_best_model) at the end of training\n",
        "    metric_for_best_model=\"accuracy\", # Metric to monitor for selecting the best model\n",
        "    report_to=\"none\",              # Disable logging to services like Weights & Biases if not needed\n",
        "    gradient_accumulation_steps=2, # Accumulate gradients over 2 steps to effectively increase batch size\n",
        "    fp16=torch.cuda.is_available(), # Enable mixed precision training if a GPU is available\n",
        ")\n",
        "print(\"Training arguments configured.\")\n",
        "\n",
        "#✅ دالة التقييم\n",
        "\n",
        "# Load the accuracy metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Computes accuracy for classification tasks.\n",
        "    Args:\n",
        "        eval_pred (tuple): A tuple containing logits and labels from the model's prediction.\n",
        "    Returns:\n",
        "        dict: A dictionary containing the computed accuracy.\n",
        "    \"\"\"\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1) # Get the predicted class by finding the argmax of logits\n",
        "    return accuracy.compute(predictions=preds, references=labels)\n",
        "\n",
        "print(\"Compute metrics function defined.\")\n",
        "\n",
        "#✅ إنشاء المدرب (Trainer)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset_dict['train'],\n",
        "    eval_dataset=dataset_dict['val'],\n",
        "    tokenizer=image_processor, # Use image_processor for preparing inputs during training/evaluation\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "print(\"Trainer initialized.\")\n",
        "\n",
        "#✅ تدريب النموذج\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "trainer.train()\n",
        "print(\"Model training complete!\")\n",
        "\n",
        "#---\n",
        "\n",
        "## ✅ اختبار النموذج على مجموعة الاختبار\n",
        "\n",
        "print(\"\\nEvaluating the model on the test set:\")\n",
        "test_results = trainer.evaluate(dataset_dict['test'])\n",
        "print(f\"Test Set Evaluation Results: {test_results}\")\n",
        "\n",
        "#---\n",
        "\n",
        "## ✅ حفظ النموذج والمعالج (اختياري)\n",
        "\n",
        "save_path = \"./vit-shark-model\"\n",
        "print(f\"\\nSaving model and image processor to {save_path}...\")\n",
        "trainer.save_model(save_path)\n",
        "image_processor.save_pretrained(save_path)\n",
        "print(\"Model and image processor saved successfully!\")"
      ],
      "metadata": {
        "id": "K2NdOW6zLAXQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}